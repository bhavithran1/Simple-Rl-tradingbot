{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\zalk\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Imports'''\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Anytrading env'''\n",
    "\n",
    "## More improvements\n",
    "## --> More features, faster obs\n",
    "## --> Not normalized data\n",
    "\n",
    "# position constant\n",
    "LONG  = 0\n",
    "SHORT = 1\n",
    "FLAT  = 2\n",
    "\n",
    "# action constant\n",
    "BUY  = 0\n",
    "SELL = 1\n",
    "HOLD = 2\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.train       = config['train']\n",
    "        self.show_trade  = config['show_trade']\n",
    "        self.window_size = config['window_size']\n",
    "\n",
    "        self.path    = config['path']\n",
    "        self.actions = [\"LONG\", \"SHORT\", \"FLAT\"]\n",
    "        self.fee = 0.0005\n",
    "\n",
    "        self.seed()\n",
    "        self._process_data()\n",
    "\n",
    "        # n_features\n",
    "        self.n_features = self.df.shape[1]\n",
    "        self.shape      = (self.window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # defines action space\n",
    "        self.action_space      = spaces.Discrete(len(self.actions))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.shape), dtype=np.float32)\n",
    "    \n",
    "    def _process_data(self):\n",
    "        \n",
    "        raw_df = pd.read_csv(self.path)\n",
    "        \n",
    "        del raw_df['date']\n",
    "        del raw_df['compsum']\n",
    "        #feature_list = raw_df.columns.values.tolist()\n",
    "        feature_list =  ['open','close','low','high','volume']\n",
    "\n",
    "        start = 0\n",
    "        end   = len(raw_df.index)\n",
    "        self.signal_features = (raw_df.loc[:, feature_list].to_numpy()[start:end])\n",
    "\n",
    "        raw_df.dropna(inplace=True) # drops Nan rows\n",
    "        self.closingPrices = raw_df['close'].values\n",
    "        self.df = raw_df[feature_list].values\n",
    "\n",
    "    def render(self, mode='human', verbose=False):\n",
    "        return None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        if self.done:\n",
    "            return self.state, self.reward, self.done, {}\n",
    "        self.reward = 0\n",
    "\n",
    "        # action comes from the agent\n",
    "        # 0 buy, 1 sell, 2 hold\n",
    "        # single position can be opened per trade\n",
    "        # valid action sequence would be\n",
    "        # LONG : buy - hold - hold - sell\n",
    "        # SHORT : sell - hold - hold - buy\n",
    "        # invalid action sequence is just considered hold\n",
    "        # (e.g.) \"buy - buy\" would be considred \"buy - hold\"\n",
    "        \n",
    "        self.action = HOLD  # hold\n",
    "        if action == BUY: # buy\n",
    "            if self.position == FLAT: # if previous position was flat\n",
    "                self.position = LONG # update position to long\n",
    "                self.action = BUY # record action as buy\n",
    "                self.entry_price = self.closingPrice # maintain entry price\n",
    "            elif self.position == SHORT: # if previous position was short\n",
    "                self.position = FLAT  # update position to flat\n",
    "                self.action = BUY # record action as buy\n",
    "                self.exit_price = self.closingPrice\n",
    "                self.reward += ((self.entry_price - self.exit_price)/self.exit_price + 1)*(1-self.fee)**2 - 1 # calculate reward\n",
    "                self.krw_balance = self.krw_balance * (1.0 + self.reward) # evaluate cumulative return in krw-won\n",
    "                self.entry_price = 0 # clear entry price\n",
    "                self.n_short += 1 # record number of short\n",
    "        elif action == 1: # vice versa for short trade\n",
    "            if self.position == FLAT:\n",
    "                self.position = SHORT\n",
    "                self.action = 1\n",
    "                self.entry_price = self.closingPrice\n",
    "            elif self.position == LONG:\n",
    "                self.position = FLAT\n",
    "                self.action = 1\n",
    "                self.exit_price = self.closingPrice\n",
    "                self.reward += ((self.exit_price - self.entry_price)/self.entry_price + 1)*(1-self.fee)**2 - 1\n",
    "                self.krw_balance = self.krw_balance * (1.0 + self.reward)\n",
    "                self.entry_price = 0\n",
    "                self.n_long += 1\n",
    "\n",
    "        # [coin + krw_won] total value evaluated in krw won\n",
    "        if(self.position == LONG):\n",
    "            temp_reward = ((self.closingPrice - self.entry_price)/self.entry_price + 1)*(1-self.fee)**2 - 1\n",
    "            new_portfolio = self.krw_balance * (1.0 + temp_reward)\n",
    "        elif(self.position == SHORT):\n",
    "            temp_reward = ((self.entry_price - self.closingPrice)/self.closingPrice + 1)*(1-self.fee)**2 - 1\n",
    "            new_portfolio = self.krw_balance * (1.0 + temp_reward)\n",
    "        else:\n",
    "            temp_reward = 0\n",
    "            new_portfolio = self.krw_balance\n",
    "\n",
    "        self.portfolio = new_portfolio\n",
    "        self.current_tick += 1\n",
    "        if(self.show_trade and self.current_tick%100 == 0):\n",
    "            print(\"Tick: {0}/ Portfolio (krw-won): {1}\".format(self.current_tick, self.portfolio))\n",
    "            print(\"Long: {0}/ Short: {1}\".format(self.n_long, self.n_short))\n",
    "        self.history.append((self.action, self.current_tick, self.closingPrice, self.portfolio, self.reward))\n",
    "        \n",
    "        self.state = self.obvs()\n",
    "        \n",
    "        info = {'portfolio':np.array([self.portfolio]),\n",
    "                                                    \"history\":self.history,\n",
    "                                                    \"n_trades\":{'long':self.n_long, 'short':self.n_short}}\n",
    "        if (self.current_tick > (self.df.shape[0]) - self.window_size-1):\n",
    "            self.done = True\n",
    "            self.reward = self.get_profit()\n",
    "            if(self.train == False):\n",
    "                np.array([info]).dump(\n",
    "                    './info/ppo_{0}_LS_{1}_{2}.info'.format(self.portfolio,\n",
    "                                                                 self.n_long,\n",
    "                                                                 self.n_short))\n",
    "        return self.state, self.reward, self.done, info\n",
    "\n",
    "    def get_profit(self):\n",
    "        if(self.position == LONG):\n",
    "            profit = ((self.closingPrice - self.entry_price)/self.entry_price + 1)*(1-self.fee)**2 - 1\n",
    "        elif(self.position == SHORT):\n",
    "            profit = ((self.entry_price - self.closingPrice)/self.closingPrice + 1)*(1-self.fee)**2 - 1\n",
    "        else:\n",
    "            profit = 0\n",
    "        return profit\n",
    "    def reset(self):\n",
    "        # self.current_tick = random.randint(0, self.df.shape[0]-800)\n",
    "        if(self.train):\n",
    "            self.current_tick = random.randint(self.window_size, self.df.shape[0] - 800)\n",
    "            # if a shape error occurs, its definetely this part here\n",
    "        else:\n",
    "            self.current_tick = self.window_size\n",
    "\n",
    "        #print(\"start episode ... {0}\" .format(self.current_tick))\n",
    "\n",
    "        # positions\n",
    "        self.n_long = 0\n",
    "        self.n_short = 0\n",
    "\n",
    "        # clear internal variables\n",
    "        self.history = [] # keep buy, sell, hold action history\n",
    "        self.krw_balance = 100 * 10000 # initial balance, u can change it to whatever u like\n",
    "        self.portfolio = float(self.krw_balance) # (coin * current_price + current_krw_balance) == portfolio\n",
    "        self.profit = 0\n",
    "        self.closingPrice = self.closingPrices[self.current_tick]\n",
    "\n",
    "        self.action = HOLD\n",
    "        self.position = FLAT\n",
    "        self.done = False\n",
    "        self.state = self.obvs()  # This is returning a obervation \n",
    "        #print(self.observation_space.shape)\n",
    "        return self.state\n",
    "\n",
    "    def obvs(self):\n",
    "        return self.signal_features[(self.current_tick-self.window_size+1):self.current_tick+1].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.42e+03 |\n",
      "|    ep_rew_mean      | -0.328   |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1        |\n",
      "|    fps              | 139      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 11360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.85e+03 |\n",
      "|    ep_rew_mean      | -0.419   |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    episodes         | 2        |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 18264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2e+03    |\n",
      "|    ep_rew_mean      | -0.453   |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3        |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 18296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.46e+03 |\n",
      "|    ep_rew_mean      | -0.557   |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 474      |\n",
      "|    total_timesteps  | 30816    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.35e+03 |\n",
      "|    ep_rew_mean      | -0.532   |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 5        |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 547      |\n",
      "|    total_timesteps  | 33576    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.73e+03 |\n",
      "|    ep_rew_mean      | -0.616   |\n",
      "|    exploration_rate | 0.965    |\n",
      "| time/               |          |\n",
      "|    episodes         | 6        |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 648      |\n",
      "|    total_timesteps  | 37072    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.03e+03 |\n",
      "|    ep_rew_mean      | -0.681   |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 7        |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 689      |\n",
      "|    total_timesteps  | 38384    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.87e+03 |\n",
      "|    ep_rew_mean      | -0.644   |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 847      |\n",
      "|    total_timesteps  | 44992    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.67e+03 |\n",
      "|    ep_rew_mean      | -0.599   |\n",
      "|    exploration_rate | 0.956    |\n",
      "| time/               |          |\n",
      "|    episodes         | 9        |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 895      |\n",
      "|    total_timesteps  | 46840    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.78e+03 |\n",
      "|    ep_rew_mean      | -0.627   |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    episodes         | 10       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 948      |\n",
      "|    total_timesteps  | 48824    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.72e+03 |\n",
      "|    ep_rew_mean      | -0.612   |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 11       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 978      |\n",
      "|    total_timesteps  | 50072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 2        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.02e+03 |\n",
      "|    ep_rew_mean      | -0.68    |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 997      |\n",
      "|    total_timesteps  | 50808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.55     |\n",
      "|    n_updates        | 25       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.86e+03 |\n",
      "|    ep_rew_mean      | -0.642   |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 13       |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 1120     |\n",
      "|    total_timesteps  | 56280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 196      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.08e+03 |\n",
      "|    ep_rew_mean      | -0.692   |\n",
      "|    exploration_rate | 0.944    |\n",
      "| time/               |          |\n",
      "|    episodes         | 14       |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 1205     |\n",
      "|    total_timesteps  | 59368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.96     |\n",
      "|    n_updates        | 293      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_config_training = {'train': True ,\"window_size\": 144, \"path\": \"D:/VS_project/ZALK/Rl_signal/MS/data/data.csv\",\"show_trade\": False}\n",
    "environment = lambda:TradingEnv(env_config_training)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Train\n",
    "    vec_env = make_vec_env(environment, n_envs=8, vec_env_cls=DummyVecEnv)\n",
    "    model = DQN('MlpPolicy',vec_env, verbose=2, batch_size=1024) \n",
    "    model.learn(total_timesteps=1000000,log_interval=1)\n",
    "    model.save(\"agent/\")#place file path\n",
    "        \n",
    "\n",
    "    # Evaluate for 100 episodes\n",
    "    env = TradingEnv(env_config_training)\n",
    "    obs = env.reset()\n",
    "    while True: \n",
    "        obs = obs[np.newaxis, ...]\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"info\", info)\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zalk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2779d0c09ec9c8b2229386c1eca40874743aef5cfa85e27601b545291980a70a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
